{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS9X-fYYmVeD",
        "outputId": "9f80c657-2c18-42cf-9ca8-450cd9c0981f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET = \"steam-reco-team-yw1204\"\n",
        "DT = \"dt=1205\"\n",
        "RAW_BASE = f\"s3a://{BUCKET}/raw/steam_kaggle/{DT}\"\n",
        "\n",
        "print(RAW_BASE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoqkPo2AmzlA",
        "outputId": "4c11231d-64ae-4ee3-cd17-a5e1df8e9395"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s3a://steam-reco-team-yw1204/raw/steam_kaggle/dt=1205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"SteamReco\")\n",
        "    .config(\n",
        "        \"spark.jars.packages\",\n",
        "        \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
        "    )\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
        "    .config(\n",
        "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
        "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
        "    )\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"64\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "WhouMPhIm6xA",
        "outputId": "0f34d53b-d78a-431d-848c-166728df9d8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bc9e89afd70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9c94dd9406ac:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SteamReco-Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = spark.read.parquet(\"s3a://steam-reco-team-yw1204/curated/reviews_clean/v=1\")\n",
        "games   = spark.read.parquet(\"s3a://steam-reco-team-yw1204/curated/games_clean/v=1\")\n",
        "users   = spark.read.parquet(\"s3a://steam-reco-team-yw1204/curated/users_clean/v=1\")\n",
        "\n",
        "reviews.printSchema()\n",
        "games.printSchema()\n",
        "users.printSchema()"
      ],
      "metadata": {
        "id": "4RzN0y7xn7AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative Filtering (CF) with ALS"
      ],
      "metadata": {
        "id": "WCCIN2rloGdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "ratings_als = (\n",
        "    reviews\n",
        "    .select(\"user_id\", \"app_id\", \"is_recommended\", \"playtime_hours\")\n",
        "    .withColumn(\n",
        "        \"rating\",\n",
        "        1.0 * F.col(\"is_recommended\").cast(\"float\") +\n",
        "        0.5 * F.log1p(F.col(\"playtime_hours\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
        "item_indexer = StringIndexer(inputCol=\"app_id\", outputCol=\"item_idx\")\n",
        "\n",
        "ratings_indexed = user_indexer.fit(ratings_als).transform(ratings_als)\n",
        "ratings_indexed = item_indexer.fit(ratings_indexed).transform(ratings_indexed)\n",
        "\n",
        "ratings_indexed = ratings_indexed.select(\n",
        "    \"user_id\", \"app_id\", \"user_idx\", \"item_idx\", \"rating\"\n",
        ")\n",
        "\n",
        "ratings_indexed.cache()\n"
      ],
      "metadata": {
        "id": "0V8kSz7tp94Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"user_idx\",\n",
        "    itemCol=\"item_idx\",\n",
        "    ratingCol=\"rating\",\n",
        "    implicitPrefs=True,\n",
        "    rank=64,\n",
        "    maxIter=10,\n",
        "    regParam=0.05,\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True,\n",
        ")\n",
        "\n",
        "als_model = als.fit(ratings_indexed)\n"
      ],
      "metadata": {
        "id": "UoEeLWOjp7pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 200 recommendations directly from ALS\n",
        "cf_recs = als_model.recommendForAllUsers(200)\n"
      ],
      "metadata": {
        "id": "r6ipP2ABqAxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cf_for_user(user_id):\n",
        "    user_idx_val = ratings_indexed.filter(F.col(\"user_id\")==user_id).select(\"user_idx\").first()\n",
        "    if user_idx_val is None:\n",
        "        return None\n",
        "    user_idx_val = user_idx_val[\"user_idx\"]\n",
        "\n",
        "    cf_df = cf_recs \\\n",
        "        .filter(F.col(\"user_idx\") == user_idx_val) \\\n",
        "        .select(F.explode(\"recommendations\").alias(\"rec\")) \\\n",
        "        .select(\n",
        "            F.col(\"rec.item_idx\").alias(\"item_idx\"),\n",
        "            F.col(\"rec.rating\").alias(\"cf_score\")\n",
        "        )\n",
        "\n",
        "    # Map item_idx back to app_id\n",
        "    item_map = ratings_indexed.select(\"item_idx\",\"app_id\").dropDuplicates()\n",
        "    cf_df = cf_df.join(item_map, \"item_idx\", \"left\").select(\"app_id\",\"cf_score\")\n",
        "\n",
        "    return cf_df\n"
      ],
      "metadata": {
        "id": "OwBdX4YvqD-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content-Based Filtering (CBF)"
      ],
      "metadata": {
        "id": "CcFRZ9t-qLcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import RegexTokenizer, HashingTF, IDF, CountVectorizer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "games_clean2 = (\n",
        "    games\n",
        "    .withColumn(\"description_clean\", F.lower(F.col(\"description\")))\n",
        "    .withColumn(\"tags_clean\", F.col(\"tags\"))\n",
        "    .na.fill({\"description_clean\": \"\"})\n",
        ")\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"description_clean\", outputCol=\"desc_words\", pattern=\"\\\\W+\")\n",
        "hashTF = HashingTF(inputCol=\"desc_words\", outputCol=\"desc_tf\", numFeatures=4096)\n",
        "idf = IDF(inputCol=\"desc_tf\", outputCol=\"desc_tfidf\")\n",
        "tag_cv = CountVectorizer(inputCol=\"tags_clean\", outputCol=\"tags_vec\")\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, hashTF, idf, tag_cv])\n",
        "model = pipeline.fit(games_clean2)\n",
        "games_vec = model.transform(games_clean2)\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"desc_tfidf\", \"tags_vec\"],\n",
        "    outputCol=\"content_vec\"\n",
        ")\n",
        "\n",
        "games_final = assembler.transform(games_vec).select(\"app_id\",\"title\",\"content_vec\")\n",
        "games_final.cache()\n"
      ],
      "metadata": {
        "id": "LMi91-qSqJCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "\n",
        "def avg_vec(vs):\n",
        "    arrs = []\n",
        "    for v in vs:\n",
        "        if isinstance(v, SparseVector):\n",
        "            arrs.append(v.toArray())\n",
        "        else:\n",
        "            arrs.append(v)\n",
        "    return DenseVector(np.mean(arrs, axis=0))\n",
        "\n",
        "avg_udf = F.udf(avg_vec)\n"
      ],
      "metadata": {
        "id": "u4cAXPu2qWWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_profile(user_id):\n",
        "    played = ratings_indexed.filter(F.col(\"user_id\")==user_id).select(\"app_id\")\n",
        "    if played.count()==0:\n",
        "        return None\n",
        "\n",
        "    game_vecs = played.join(games_final, \"app_id\").select(\"content_vec\")\n",
        "    profile = game_vecs.agg(avg_udf(F.collect_list(\"content_vec\")).alias(\"profile_vec\")).first()\n",
        "    return profile[\"profile_vec\"]\n"
      ],
      "metadata": {
        "id": "7qj2JQLwqaA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.function import udf\n",
        "\n",
        "def cosine(a, b):\n",
        "    import numpy as np\n",
        "    if isinstance(a, SparseVector): a = a.toArray()\n",
        "    if isinstance(b, SparseVector): b = b.toArray()\n",
        "    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)+1e-9))\n",
        "\n",
        "cosine_udf = F.udf(cosine, DoubleType())\n"
      ],
      "metadata": {
        "id": "u-onoeKUqej_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cb_scores(user_profile, candidate_df):\n",
        "    return (\n",
        "        candidate_df\n",
        "        .join(games_final, \"app_id\")\n",
        "        .withColumn(\"cb_score\", cosine_udf(F.lit(user_profile), \"content_vec\"))\n",
        "    )\n"
      ],
      "metadata": {
        "id": "FEgv-0ymqz1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Recommender"
      ],
      "metadata": {
        "id": "gHKn9CvXq3bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "def recommend_games(user_id, top_n=20, alpha_cf=0.4, alpha_cb=0.6):\n",
        "\n",
        "    cf_df = get_cf_for_user(user_id)\n",
        "    if cf_df is None or cf_df.count()==0:\n",
        "        print(\"No CF data for this user\")\n",
        "        return None\n",
        "\n",
        "    user_profile = get_user_profile(user_id)\n",
        "    if user_profile is None:\n",
        "        print(\"User has no content profile\")\n",
        "        return None\n",
        "\n",
        "    played = ratings_indexed.filter(F.col(\"user_id\")==user_id)\\\n",
        "                            .select(\"app_id\").distinct()\n",
        "\n",
        "    cf_filtered = cf_df.join(played, \"app_id\", \"leftanti\")\n",
        "\n",
        "    cb_scored = compute_cb_scores(user_profile, cf_filtered)\n",
        "\n",
        "    w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "    hybrid = (\n",
        "        cb_scored\n",
        "        .withColumn(\"cf_norm\", F.col(\"cf_score\")/F.max(\"cf_score\").over(w))\n",
        "        .withColumn(\"cb_norm\", F.col(\"cb_score\")/F.max(\"cb_score\").over(w))\n",
        "        .withColumn(\"hybrid_score\",\n",
        "             alpha_cf*F.col(\"cf_norm\") + alpha_cb*F.col(\"cb_norm\")\n",
        "        )\n",
        "        .join(games.select(\"app_id\",\"title\",\"tags\"), \"app_id\", \"left\")\n",
        "        .orderBy(F.col(\"hybrid_score\").desc())\n",
        "        .limit(top_n)\n",
        "    )\n",
        "\n",
        "    hybrid.show(truncate=False)\n",
        "    return hybrid\n"
      ],
      "metadata": {
        "id": "aA02V7w6q19H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}